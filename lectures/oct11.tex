\section{October 11 Lecture}

Final projects: think about what brought you to study Applied Math. What problems do you like to solve? We'll find some linear algebra componenet to it.

Last class:

\paragraph{Row space of $A$} $A \rightarrow U$, the `$r$' non zero rows are a basis for the row space $C(A^T)$. It has dimension $r$ and it is a subspace of $\bbR$. The row space of $U$ is the same as the row space of $A$, since they only differ by linear combinations of rows.

The row space of $A$ and $U$ have the same basis.

\paragraph{Null space of $A$} $N(A) = N(U)$. If $r$ rows are linearly independent $\Rightarrow$ there are $(n-r)$ free variables the dimension of $N(A) = n-r$. 

Null space definition:

\[
  x \in R^n s.t. Ax = 0
\]

\paragraph{Column space of $A$} $C(A)$. When we transform $A$ to $U$, the first non-zero elements' index in each row determines which variable of $x$ will be a pivot variable, suggesting that those indices determine the column space of $A$. The column space of $A$ is \textit{not} the same column space of $U$. Dimension of $C(A) = r$. $C(A)$ is a subspace of $\bbR^m$.

\paragraph{Left null space}

\[
  \begin{bmatrix}
    y_1 & y_2 & \cdots & y_m
  \end{bmatrix}
  \begin{bmatrix}
    \\ A^T \\ \\
  \end{bmatrix}_{m \times n} =
  \underbrace{\begin{bmatrix}
    0 & \cdots & 0
  \end{bmatrix}}_n
  \text{ it is a subspace of } \bbR^m
\]

$N(A^T)$ is $A^T y = 0$. If $y$ is in the null space of $A^T$, then $y^T$ is in the left null space of $A$ ($y^TA = 0$).

\subsection{Fundamental Theorem of Linear Algebra}

\begin{itemize}
  \item $\dim(C(A)) = r$
  \item $\dim(C(A^T)) = r$
  \item $\dim(N(A)) = n-r$
  \item $\dim(N(A^T)) = m-r$
\end{itemize}

$Ax=b$. Case: $(m \le n)$

\[
  \begin{bmatrix}
    & & & & & & \\
    & & & A & & & \\
    & & & & & & 
  \end{bmatrix}_{m \times n}
  \begin{bmatrix}
    \\ \\ \\ x \\ \\ \\ \\
  \end{bmatrix}_{n \times 1} = 
  \begin{bmatrix}
    \\ \\ b \\ \\ \\
  \end{bmatrix}_{m \times 1}
\]

\paragraph{Existance:} If $A$ has the maximum number of linearly independent rows($=m$) $A$ is said to have full ``row'' rank. There exists at least one solution for any $b$

In this case, $A$ has a right inverse. An example:

\begin{align*}
  A &= 
  \begin{bmatrix}
    4 & 0 & 0 \\ 0 & 5 & 0
  \end{bmatrix}
  C =
  \begin{bmatrix}
    1/4 & 0 \\ 0 & 1/5 \\ \alpha & \beta
  \end{bmatrix} \\
  AC &= \begin{bmatrix}
    1 & 0 \\ 0 & 1
  \end{bmatrix}
\end{align*}

In this example, the right inverse is not unique! Any values of $\alpha$ and $\beta$ will work.
  
\begin{align*}
  ACb &= b \\
  Ax &= b
  x &= Cb
\end{align*}

\paragraph{Uniqueness} $m \ge n$

\[
  \begin{bmatrix}
    & & \\
    & & \\
    & A & \\
    & & \\
    & & \\
  \end{bmatrix}_{m \times n}
  \begin{bmatrix}
    \\ x \\ \\
  \end{bmatrix}_{n \times 1} = 
  \begin{bmatrix}
    \\ \\ \\ \\ b \\ \\ \\ \\ \\
  \end{bmatrix}_{m \times 1}
\]

If all columns of $A$ are linearly independent, $A$ is said to be full ``column'' rank.

If $b \in C(A) \exists$ 1 unique solution. If $b \!in C(A)$, then no solution.

$\bbR^n$: normed vector space (with an innter product)

\[
  \<x,y\> = \sum_{i=1}^n x_i y_i
\]
\[
  \norm{x}^2 = \<x,x\> = x^T x
\]

Interesting property:

\[
  \<x,y\> = \norm{x}\norm{y}\cos \theta
\]

$x$ and $y$ are said to be orthogonal if $\<x,y\> = 0$. If we have $k$ non-zero vectors ($v_1,\cdots, v_k$) are mutually orthogonal, they are linearly independent. Then we can say that there is only one combination that satisfies the following:

\[
  c_1v_1 + c_2v_2 + \cdots + c_kv_k=0
\]

All $c_i$ must be 0. Proof:

\begin{align*}
  v_1^T(c_1v_1 + c_2v_2 + \cdots + c_kv_k)&=0 \\
  v_1^Tc_1v_1 + v_1^Tc_2v_2 + \cdots + v_1^Tc_kv_k &=0 \\
  c_1 \norm{v_1}^2 &= 0
\end{align*}

Repeat for all $v_i \Rightarrow c_i = 0 \forall i \Rightarrow \{v_k\}$ are linearly independent.

\[
  e_i = \begin{bmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
  \end{bmatrix} \text{ 1 at the } i\text{th element}
\]

$\{e_i\}$ is an orthonormal basis of $\bbR^n$. Any vector in $\bbR^n$ can be generated as a linear combination of them. If every vector in a subspace $V$ is orthogonal to every vector in subspace $W \Rightarrow V$ and $W$ are said to be orthogonal subspaces.

\begin{enumerate}
  \item The row space of $A$, $C(A^T)$ is the orthogonal complement to $N(A)$.
  \item The column space of $A$, $C(A)$ is the orthogonal complement to the left null space $N(A^T)$.
\end{enumerate}

\begin{enumerate}
  \item Why is this true? By definition, the null space are vectors $x$ such that $Ax = 0$. If $A$ is $m \times n$. This means that the inner product of every row of $A$ with $x$ must be 0. In other words, $x$ is orthogonal to every row of $A$. The rows define the row space $C(A^T)$, so (1) is true.
  \item The left null space is defined as $y$ such that $y^TA = 0$. If $A$ is $m \times n$, then the inner product of $y$ and each of the $n$ columns of $A$ must be 0.
\end{enumerate}
